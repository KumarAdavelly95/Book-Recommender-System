# -*- coding: utf-8 -*-
"""SNA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WbR6hdaIdCLyPKshSp6AvrcSLe4mD6ha
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
#Change current working directory to gdrive         
# %cd /gdrive

trainfile = r'/gdrive/My Drive/509/SNA/amazonBooks.csv'

import networkx
from operator import itemgetter
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Read the data from amazon-books.csv into amazonBooks dataframe;
amazonBooks = pd.read_csv(trainfile)

#setting the index to asin 
amazonBooks.set_index('Unnamed: 0', inplace=True)
amazonBooks.sort_values(['DegreeCentrality'],ascending=False)

# Read the data from amazon-books-copurchase.adjlist;
# assign it to copurchaseGraph weighted Graph;
# node = ASIN, edge= copurchase, edge weight = category similarity
#fhr=open("amazon-books-copurchase.edgelist", 'rb')

fhr = r'/gdrive/My Drive/509/SNA/amazon-books-copurchase.edgelist'
copurchaseGraph=networkx.read_weighted_edgelist(fhr)
#fhr.close()

# Now let's assume a person is considering buying the following book;
# what else can we recommend to them based on copurchase behavior 
# we've seen from other users?
print ("Looking for Recommendations for Customer Purchasing this Book:")
print ("--------------------------------------------------------------")
purchasedAsin = '0805047905'

# Let's first get some metadata associated with this book
print ("ASIN = ", purchasedAsin) 
print ("Title = ", amazonBooks.loc[purchasedAsin,'Title'])
print ("Category = ", amazonBooks.loc[purchasedAsin,'Categories'])
print ("SalesRank = ", amazonBooks.loc[purchasedAsin,'SalesRank'])
print ("TotalReviews = ", amazonBooks.loc[purchasedAsin,'TotalReviews'])
print ("AvgRating = ", amazonBooks.loc[purchasedAsin,'AvgRating'])
print ("DegreeCentrality = ", amazonBooks.loc[purchasedAsin,'DegreeCentrality'])
print ("ClusteringCoeff = ", amazonBooks.loc[purchasedAsin,'ClusteringCoeff'])

# Now let's look at the ego network associated with purchasedAsin in the
# copurchaseGraph - which is esentially comprised of all the books 
# that have been copurchased with this book in the past
# (1) YOUR CODE HERE: 
#     Get the depth-1 ego network of purchasedAsin from copurchaseGraph,
#     and assign the resulting graph to purchasedAsinEgoGraph.
purchasedAsinEgoGraph = networkx.Graph()



n = purchasedAsin

purchasedAsinEgoGraph = networkx.ego_graph(copurchaseGraph, n, radius=1)

'''plt.figure(figsize=(15,15))
pos = networkx.spring_layout(purchasedAsinEgoGraph)
networkx.draw_networkx_nodes(purchasedAsinEgoGraph,pos,node_size=600)
networkx.draw_networkx_labels(purchasedAsinEgoGraph,pos,font_size=12)
networkx.draw_networkx_edges(purchasedAsinEgoGraph,pos)
plt.axis('off')
plt.show()'''

# Next, recall that the edge weights in the copurchaseGraph is a measure of
# the similarity between the books connected by the edge. So we can use the 
# island method to only retain those books that are highly simialr to the 
# purchasedAsin
# (2) YOUR CODE HERE: 
#     Use the island method on purchasedAsinEgoGraph to only retain edges with 
#     threshold >= 0.5, and assign resulting graph to purchasedAsinEgoTrimGraph
threshold = 0.5
purchasedAsinEgoTrimGraph = networkx.Graph()

for f,t,e in purchasedAsinEgoGraph.edges(data=True):
  if e['weight'] >= threshold:
    purchasedAsinEgoTrimGraph.add_edge(f,t,weight=e['weight'])

# Next, recall that given the purchasedAsinEgoTrimGraph you constructed above, 
# you can get at the list of nodes connected to the purchasedAsin by a single 
# hop (called the neighbors of the purchasedAsin) 
# (3) YOUR CODE HERE: 
#     Find the list of neighbors of the purchasedAsin in the 
#     purchasedAsinEgoTrimGraph, and assign it to purchasedAsinNeighbors
purchasedAsinNeighbors = [i for i in purchasedAsinEgoTrimGraph.neighbors(purchasedAsin)]
purchasedAsinNeighbors

len(purchasedAsinNeighbors)

# Next, let's pick the Top Five book recommendations from among the 
# purchasedAsinNeighbors based on one or more of the following data of the 
# neighboring nodes: SalesRank, AvgRating, TotalReviews, DegreeCentrality, 
# and ClusteringCoeff
# (4) YOUR CODE HERE: 
#     Note that, given an asin, you can get at the metadata associated with  
#     it using amazonBooks (similar to lines 29-36 above).
#     Now, come up with a composite measure to make Top Five book 
#     recommendations based on one or more of the following metrics associated 
#     with nodes in purchasedAsinNeighbors: SalesRank, AvgRating, 
#     TotalReviews, DegreeCentrality, and ClusteringCoeff. Feel free to compute
#     and include other measures if you like.
#     YOU MUST come up with a composite measure.
#     DO NOT simply make recommendations based on sorting!!!
#     Also, remember to transform the data appropriately using 
#     sklearn preprocessing so the composite measure isn't overwhelmed 
#     by measures which are on a higher scale.



#create copurchased dataframe
copurchased_df = amazonBooks.filter(items=purchasedAsinNeighbors, axis=0)

#convert sales rank to a function of logx
copurchased_df['LogRank']=copurchased_df['SalesRank'].apply(lambda x: np.log(1/x))

#convert columns with skewness
#copurchased_df['AdjReviews']=copurchased_df['TotalReviews'].apply(lambda x: np.cbrt(x) if (copurchased_df['TotalReviews'].skew()>0.5) else (np.square(x) if (copurchased_df['TotalReviews'].skew()<-0.5) else x))
#copurchased_df['AdjRatings']=copurchased_df['AvgRating'].apply(lambda x: np.cbrt(x) if (copurchased_df['AvgRating'].skew()>0.5) else (np.square(x) if (copurchased_df['AvgRating'].skew()<-0.5) else x))

#reset index to perform concatenation
copurchased_df1 = copurchased_df.reset_index()

#min max scale the necessary columns
numericvars = ['AvgRating','LogRank','DegreeCentrality','TotalReviews']
from sklearn.preprocessing import MinMaxScaler
mms = MinMaxScaler(feature_range=(0.01,1.00))
dfnumss = pd.DataFrame(mms.fit_transform(copurchased_df1[numericvars]), columns=['mms_'+x for x in numericvars])

#concat the dataframes and set Asin as index
dfnew = pd.concat([copurchased_df1,dfnumss], verify_integrity=True,axis=1)
dfnew.set_index('Unnamed: 0', inplace=True)

dfnew

#drop unnecessary columns
dfnew.drop(columns=['SalesRank','TotalReviews','AvgRating','DegreeCentrality','LogRank'], inplace=True)

def top_picks(df):

  if dfnew['mms_TotalReviews'].skew()>0.5:
    e=np.sqrt(dfnew['mms_TotalReviews'])
  elif dfnew['mms_TotalReviews'].skew()<-0.5:
    e=np.square(dfnew['mms_TotalReviews'])
  else:
    e=df['mms_TotalReviews']

  if dfnew['mms_AvgRating'].skew()>0.5:
    b=np.sqrt(dfnew['mms_AvgRating'])
  elif dfnew['mms_AvgRating'].skew()<-0.5:
    b=np.square(dfnew['mms_AvgRating'])
  else:
    b=dfnew['mms_AvgRating']


  a=dfnew['ClusteringCoeff']+0.01
  c=dfnew['mms_LogRank']*2
  d=dfnew['mms_DegreeCentrality']
  

  return (a*b*c*d*e)*1000
  

dfnew['score']= top_picks(dfnew)

dfnew

# Print Top 5 recommendations (ASIN, and associated Title, Sales Rank, 
# TotalReviews, AvgRating, DegreeCentrality, ClusteringCoeff)
# (5) YOUR CODE HERE:  
dfnew.sort_values(['score'],ascending=False)[0:5]

